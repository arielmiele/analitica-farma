"""
M√≥dulo de visualizaci√≥n para Validaci√≥n Cruzada - Anal√≠tica Farma
Contiene funciones para mostrar resultados, diagn√≥sticos y visualizaciones interactivas.
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go


def mostrar_resultados_analisis(resultados_curvas, modelo, resultados_benchmarking):
    """Muestra los resultados del an√°lisis de validaci√≥n."""
    
    # Verificar si hay error en los resultados
    if 'error' in resultados_curvas:
        st.error(f"‚ùå {resultados_curvas['error']}")
        if 'solucion' in resultados_curvas:
            st.info(f"üí° **Sugerencia:** {resultados_curvas['solucion']}")
        return
    
    # An√°lisis de diagn√≥stico
    diagnostico = resultados_curvas.get('diagnostico', {})
    metricas = resultados_curvas.get('metricas_principales', {})
    cv_results_completos = resultados_curvas.get('cv_results_completos', {})
    
    st.success("‚úÖ An√°lisis de validaci√≥n cruzada completado")
    
    # 1. Mostrar diagn√≥stico principal
    mostrar_diagnostico_principal(diagnostico, modelo['nombre'])
    
    # 2. Mostrar curvas de aprendizaje (nueva funcionalidad)
    mostrar_curvas_aprendizaje_interactivas(resultados_curvas, modelo['nombre'])
    
    # 3. Mostrar m√©tricas y puntuaciones CV mejoradas
    mostrar_metricas_validacion_mejoradas(metricas, cv_results_completos, resultados_curvas.get('tipo_problema'))
    
    # 4. Mostrar informaci√≥n de datos
    mostrar_informacion_datos(resultados_curvas.get('datos_disponibles', {}))
    
    # 5. Mostrar recomendaciones
    from .recomendaciones import mostrar_recomendaciones_mejora
    mostrar_recomendaciones_mejora(diagnostico, modelo, resultados_benchmarking.get('tipo_problema', 'clasificacion'))


def mostrar_metricas_validacion(metricas, cv_scores, tipo_problema):
    """Muestra las m√©tricas de validaci√≥n y puntuaciones CV."""
    if not metricas and not cv_scores:
        st.info("üìä No hay m√©tricas de validaci√≥n disponibles")
        return
    
    st.subheader("üìà M√©tricas de Validaci√≥n")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**M√©tricas principales:**")
        for metrica, valor in metricas.items():
            if isinstance(valor, (int, float)):
                st.metric(metrica.replace('_', ' ').title(), f"{valor:.4f}")
            else:
                st.write(f"- **{metrica.replace('_', ' ').title()}:** {valor}")
    
    with col2:
        if cv_scores:
            st.write("**Validaci√≥n Cruzada:**")
            cv_mean = np.mean(cv_scores)
            cv_std = np.std(cv_scores)
            st.metric("Media CV", f"{cv_mean:.4f}")
            st.metric("Desviaci√≥n Est√°ndar CV", f"{cv_std:.4f}")
            
            # Mostrar distribuci√≥n de scores
            with st.expander("üìä Distribuci√≥n de puntuaciones CV"):
                st.bar_chart(cv_scores)


def mostrar_informacion_datos(datos_disponibles):
    """Muestra informaci√≥n sobre los datos utilizados."""
    if not datos_disponibles:
        return
    
    with st.expander("üìä Informaci√≥n de datos", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            st.write(f"**Forma de X_test:** {datos_disponibles.get('X_test_shape', 'N/A')}")
            st.write(f"**Forma de y_test:** {datos_disponibles.get('y_test_shape', 'N/A')}")
        
        with col2:
            st.write(f"**Total de filas:** {datos_disponibles.get('total_filas', 'N/A')}")
            st.write(f"**% datos de prueba:** {datos_disponibles.get('porcentaje_test', 'N/A')}%")


def mostrar_diagnostico_principal(diagnostico, nombre_modelo):
    """Muestra el diagn√≥stico principal del modelo."""
    st.subheader(f"üîç Diagn√≥stico de {nombre_modelo}")
    
    # Obtener informaci√≥n del diagn√≥stico
    overfitting = diagnostico.get('overfitting', 'desconocido')
    underfitting = diagnostico.get('underfitting', 'desconocido')
    varianza_cv = diagnostico.get('varianza_cv', 0)
    mensaje = diagnostico.get('mensaje', 'An√°lisis no disponible')
    nivel_confianza = diagnostico.get('nivel_confianza', 'bajo')
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        # Mostrar diagn√≥stico principal
        if overfitting == 'posible':
            st.warning("‚ö†Ô∏è **POSIBLE OVERFITTING**")
            st.markdown("""
            El modelo podr√≠a estar sobreajustado a los datos de entrenamiento. 
            Esto significa que puede haber memorizado patrones espec√≠ficos que 
            no se generalizan bien a datos nuevos.
            """)
        elif underfitting == 'posible':
            st.warning("‚ö†Ô∏è **POSIBLE UNDERFITTING**")
            st.markdown("""
            El modelo parece ser demasiado simple para capturar los patrones 
            en los datos. Considere aumentar la complejidad del modelo.
            """)
        elif overfitting == 'improbable' and underfitting == 'improbable':
            st.success("‚úÖ **MODELO BALANCEADO**")
            st.markdown("""
            El modelo muestra un comportamiento equilibrado sin signos evidentes 
            de overfitting o underfitting.
            """)
        else:
            st.info("üìä **AN√ÅLISIS INCOMPLETO**")
            st.markdown("""
            No hay suficiente informaci√≥n para determinar con certeza el 
            comportamiento del modelo.
            """)
        
        # Mostrar mensaje detallado
        st.write(f"**An√°lisis:** {mensaje}")
    
    with col2:
        st.metric("Varianza CV", f"{varianza_cv:.4f}")
        
        # Interpretaci√≥n de la varianza
        if varianza_cv > 0.1:
            st.write("üî¥ Alta varianza")
        elif varianza_cv < 0.03:
            st.write("üü¢ Baja varianza")
        else:
            st.write("üü° Varianza normal")
    
    with col3:
        st.metric("Confianza", nivel_confianza.title())
        
        # C√≥digo de colores para confianza
        if nivel_confianza == 'alto':
            st.write("üü¢ Diagn√≥stico fiable")
        elif nivel_confianza == 'medio':
            st.write("üü° Diagn√≥stico parcial")
        else:
            st.write("üî¥ Diagn√≥stico limitado")
    
    # Mostrar informaci√≥n adicional en expander
    with st.expander("‚ÑπÔ∏è Detalles t√©cnicos", expanded=False):
        st.write("**Estado del diagn√≥stico:**")
        st.write(f"- Overfitting: {overfitting}")
        st.write(f"- Underfitting: {underfitting}")
        st.write(f"- Varianza en validaci√≥n cruzada: {varianza_cv:.6f}")
        st.write(f"- Nivel de confianza: {nivel_confianza}")
        
        if varianza_cv > 0:
            st.write("**Interpretaci√≥n de varianza:**")
            if varianza_cv > 0.1:
                st.write("La alta varianza sugiere que el modelo es inconsistente entre diferentes subconjuntos de datos.")
            elif varianza_cv < 0.03:
                st.write("La baja varianza indica que el modelo es consistente entre diferentes subconjuntos de datos.")
            else:
                st.write("La varianza est√° en un rango normal, indicando un comportamiento estable del modelo.")


def mostrar_curvas_aprendizaje_interactivas(resultados_curvas, nombre_modelo):
    """Muestra las curvas de aprendizaje con visualizaci√≥n interactiva."""
    st.subheader("üìà An√°lisis de Curvas de Aprendizaje")
    
    # Verificar si hay curvas de aprendizaje reales disponibles
    learning_curves = resultados_curvas.get('learning_curves', {})
    cv_scores = resultados_curvas.get('cv_scores', [])
    
    if learning_curves and not learning_curves.get('error'):
        st.success("‚úÖ Curvas de aprendizaje generadas con scikit-learn")
        
        # Mostrar m√©tricas principales de las curvas
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            final_gap = learning_curves.get('final_gap', 0)
            st.metric("Gap Final", f"{final_gap:.4f}")
            if final_gap > 0.1:
                st.write("üî¥ Alto overfitting")
            elif final_gap < 0.03:
                st.write("üü¢ Buen ajuste")
            else:
                st.write("üü° Overfitting moderado")
        
        with col2:
            max_gap = learning_curves.get('max_gap', 0)
            st.metric("Gap M√°ximo", f"{max_gap:.4f}")
        
        with col3:
            gap_trend = learning_curves.get('gap_trend', 'estable')
            st.metric("Tendencia", gap_trend.title())
        
        with col4:
            scoring_metric = learning_curves.get('scoring_metric', 'N/A')
            st.metric("M√©trica", scoring_metric.upper())
        
        # Crear gr√°fico de curvas de aprendizaje
        crear_grafico_curvas_aprendizaje(learning_curves, nombre_modelo)
        
        # Interpretaci√≥n del gap de overfitting
        mostrar_interpretacion_gap(learning_curves)
        
    elif cv_scores:
        st.info("üìä An√°lisis basado en validaci√≥n cruzada disponible")
        
        # Mostrar estad√≠sticas de CV
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Media CV", f"{np.mean(cv_scores):.4f}")
        with col2:
            st.metric("Desv. Est√°ndar CV", f"{np.std(cv_scores):.4f}")
        with col3:
            st.metric("Rango CV", f"{np.max(cv_scores) - np.min(cv_scores):.4f}")
        
        # Gr√°fico de barras de puntuaciones CV
        st.bar_chart(cv_scores)
        
    else:
        st.warning("‚ö†Ô∏è No hay datos de validaci√≥n cruzada disponibles para mostrar curvas de aprendizaje")
        st.info("üí° Ejecute el an√°lisis de validaci√≥n cruzada para ver las curvas completas")


def mostrar_interpretacion_detallada(interpretacion, diagnostico):
    """Muestra interpretaci√≥n detallada del an√°lisis."""
    if not interpretacion and not diagnostico:
        return
        
    st.subheader("üìã Interpretaci√≥n Detallada")
    
    # Mostrar resumen del an√°lisis
    mensaje = diagnostico.get('mensaje', 'No hay mensaje de diagn√≥stico disponible')
    st.info(f"**Resumen:** {mensaje}")
    
    # Mostrar recomendaciones si est√°n disponibles
    if 'recomendaciones' in interpretacion:
        st.write("**Recomendaciones t√©cnicas:**")
        for rec in interpretacion['recomendaciones']:
            st.write(f"‚Ä¢ {rec}")
    
    # Detalles t√©cnicos en expander
    with st.expander("üîç Detalles t√©cnicos avanzados", expanded=False):
        st.write("**Informaci√≥n del diagn√≥stico:**")
        for clave, valor in diagnostico.items():
            if clave != 'mensaje':
                st.write(f"- **{clave.replace('_', ' ').title()}:** {valor}")


def crear_grafico_distribucion_cv(cv_scores, nombre_modelo):
    """Crea un gr√°fico de distribuci√≥n de puntuaciones CV."""
    if not cv_scores:
        return None
    
    fig = go.Figure()
    
    # Agregar histograma
    fig.add_trace(go.Histogram(
        x=cv_scores,
        nbinsx=min(10, len(cv_scores)),
        name='Distribuci√≥n CV',
        marker_color='lightblue',
        opacity=0.7
    ))
    
    # L√≠nea de la media
    media = np.mean(cv_scores)
    fig.add_vline(
        x=media, 
        line_dash="dash", 
        line_color="red",
        annotation_text=f"Media: {media:.3f}"
    )
    
    fig.update_layout(
        title=f"Distribuci√≥n de Puntuaciones CV - {nombre_modelo}",
        xaxis_title="Puntuaci√≥n",
        yaxis_title="Frecuencia",
        showlegend=False
    )
    
    return fig


def mostrar_comparacion_modelos(resultados_comparacion):
    """Muestra comparaci√≥n entre m√∫ltiples modelos."""
    if not resultados_comparacion:
        return
        
    st.subheader("‚öñÔ∏è Comparaci√≥n de Modelos")
    
    # Crear tabla comparativa
    df_comparacion = pd.DataFrame(resultados_comparacion)
    st.dataframe(df_comparacion, use_container_width=True)
    
    # Gr√°fico de comparaci√≥n
    if len(df_comparacion) > 1:
        metric_cols = [col for col in df_comparacion.columns if col not in ['Modelo', 'Tipo']]
        
        for metric in metric_cols:
            if df_comparacion[metric].dtype in ['float64', 'int64']:
                fig = go.Figure()
                fig.add_trace(go.Bar(
                    x=df_comparacion['Modelo'],
                    y=df_comparacion[metric],
                    name=metric,
                    marker_color='lightgreen'
                ))
                
                fig.update_layout(
                    title=f"Comparaci√≥n: {metric}",
                    xaxis_title="Modelo",
                    yaxis_title=metric,
                    showlegend=False
                )
                
                st.plotly_chart(fig, use_container_width=True)


def crear_grafico_curvas_aprendizaje(learning_curves, nombre_modelo):
    """Crea un gr√°fico interactivo de curvas de aprendizaje."""
    # Extraer datos de las curvas
    train_sizes = learning_curves.get('train_sizes', [])
    train_scores_mean = learning_curves.get('train_scores_mean', [])
    train_scores_std = learning_curves.get('train_scores_std', [])
    validation_scores_mean = learning_curves.get('validation_scores_mean', [])
    validation_scores_std = learning_curves.get('validation_scores_std', [])
    overfitting_gap = learning_curves.get('overfitting_gap', [])
    
    try:
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        
        if not train_sizes or not train_scores_mean:
            st.warning("‚ö†Ô∏è Datos de curvas de aprendizaje incompletos")
            return
        
        # Crear subplots
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=(
                '<b>Curvas de Aprendizaje</b><br><span style="font-size:12px; color:gray;">Entrenamiento vs Validaci√≥n</span>',
                '<b>Gap de Overfitting (Entrenamiento - Validaci√≥n)</b>'
            ),
            vertical_spacing=0.18  # Aumenta el espacio vertical entre los gr√°ficos
        )
        
        # Gr√°fico 1: Curvas de aprendizaje
        # Curva de entrenamiento
        fig.add_trace(
            go.Scatter(
                x=train_sizes,
                y=train_scores_mean,
                mode='lines+markers',
                name='Entrenamiento',
                line=dict(color='blue', width=2),
                marker=dict(size=6),
                showlegend=True
            ),
            row=1, col=1
        )
        
        # Banda de confianza entrenamiento
        if train_scores_std:
            train_upper = [m + s for m, s in zip(train_scores_mean, train_scores_std)]
            train_lower = [m - s for m, s in zip(train_scores_mean, train_scores_std)]
            
            fig.add_trace(
                go.Scatter(
                    x=train_sizes + train_sizes[::-1],
                    y=train_upper + train_lower[::-1],
                    fill='toself',
                    fillcolor='rgba(0,100,200,0.2)',
                    line=dict(color='rgba(255,255,255,0)'),
                    showlegend=False,
                    name='Banda entrenamiento'
                ),
                row=1, col=1
            )
        
        # Curva de validaci√≥n
        fig.add_trace(
            go.Scatter(
                x=train_sizes,
                y=validation_scores_mean,
                mode='lines+markers',
                name='Validaci√≥n',
                line=dict(color='red', width=2),
                marker=dict(size=6),
                showlegend=True
            ),
            row=1, col=1
        )
        
        # Banda de confianza validaci√≥n
        if validation_scores_std:
            val_upper = [m + s for m, s in zip(validation_scores_mean, validation_scores_std)]
            val_lower = [m - s for m, s in zip(validation_scores_mean, validation_scores_std)]
            
            fig.add_trace(
                go.Scatter(
                    x=train_sizes + train_sizes[::-1],
                    y=val_upper + val_lower[::-1],
                    fill='toself',
                    fillcolor='rgba(200,50,50,0.2)',
                    line=dict(color='rgba(255,255,255,0)'),
                    showlegend=False,
                    name='Banda validaci√≥n'
                ),
                row=1, col=1
            )
        
        # Gr√°fico 2: Gap de overfitting
        if overfitting_gap:
            fig.add_trace(
                go.Scatter(
                    x=train_sizes,
                    y=overfitting_gap,
                    mode='lines+markers',
                    name='Gap Overfitting',
                    line=dict(color='orange', width=2),
                    marker=dict(size=6),
                    showlegend=False
                ),
                row=2, col=1
            )
            
            # L√≠nea de referencia en 0.1 (umbral de overfitting)
            fig.add_trace(
                go.Scatter(
                    x=[min(train_sizes), max(train_sizes)],
                    y=[0.1, 0.1],
                    mode='lines',
                    line=dict(color='red', dash='dash'),
                    name='Umbral overfitting (0.1)',
                    showlegend=False
                ),
                row=2, col=1
            )
        
        # Configurar layout
        fig.update_layout(
            title=f"An√°lisis de Curvas de Aprendizaje - {nombre_modelo}",
            height=700,
            showlegend=True,
            template="plotly_white"
        )
        
        # Etiquetas de ejes
        fig.update_xaxes(title_text="Tama√±o del conjunto de entrenamiento", row=1, col=1)
        fig.update_xaxes(title_text="Tama√±o del conjunto de entrenamiento", row=2, col=1)
        fig.update_yaxes(title_text="Puntuaci√≥n", row=1, col=1)
        fig.update_yaxes(title_text="Gap (Train - Val)", row=2, col=1)
        
        # Mostrar gr√°fico
        st.plotly_chart(fig, use_container_width=True)
        
    except Exception as e:
        st.error(f"‚ùå Error al crear gr√°fico de curvas de aprendizaje: {str(e)}")
        st.info("üí° Mostrando datos tabulares como alternativa")
        
        # Mostrar datos en formato tabular como fallback
        if train_sizes and train_scores_mean and validation_scores_mean:
            df_curves = pd.DataFrame({
                'Tama√±o_Entrenamiento': train_sizes,
                'Score_Entrenamiento': train_scores_mean,
                'Score_Validacion': validation_scores_mean,
                'Gap_Overfitting': overfitting_gap if overfitting_gap else [0] * len(train_sizes)
            })
            st.dataframe(df_curves, use_container_width=True)


def mostrar_interpretacion_gap(learning_curves):
    """Muestra interpretaci√≥n detallada del gap de overfitting."""
    st.subheader("üîç Interpretaci√≥n del Gap de Overfitting")
    
    final_gap = learning_curves.get('final_gap', 0)
    max_gap = learning_curves.get('max_gap', 0)
    gap_trend = learning_curves.get('gap_trend', 'estable')
    
    # Interpretaci√≥n principal
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if final_gap > 0.15:
            st.error("üö® **OVERFITTING SEVERO DETECTADO**")
            st.markdown("""
            **Diagn√≥stico:** El modelo muestra signos claros de sobreajuste.
            
            **Implicaciones:**
            - El modelo memoriza los datos de entrenamiento en lugar de aprender patrones generalizables
            - El rendimiento en datos nuevos ser√° significativamente menor
            - Existe una brecha considerable entre el rendimiento de entrenamiento y validaci√≥n
            """)
            
        elif final_gap > 0.1:
            st.warning("‚ö†Ô∏è **OVERFITTING MODERADO**")
            st.markdown("""
            **Diagn√≥stico:** El modelo muestra tendencias de sobreajuste que requieren atenci√≥n.
            
            **Implicaciones:**
            - Hay espacio para mejorar la generalizaci√≥n del modelo
            - El rendimiento en datos nuevos podr√≠a ser sub-√≥ptimo
            - Se recomienda aplicar t√©cnicas de regularizaci√≥n
            """)
            
        elif final_gap > 0.05:
            st.info("‚ÑπÔ∏è **AJUSTE NORMAL**")
            st.markdown("""
            **Diagn√≥stico:** El modelo muestra un comportamiento normal con un gap m√≠nimo aceptable.
            
            **Implicaciones:**
            - El modelo generaliza razonablemente bien
            - La diferencia entre entrenamiento y validaci√≥n est√° en un rango aceptable
            - El modelo est√° funcionando como se espera
            """)
            
        else:
            st.success("‚úÖ **EXCELENTE GENERALIZACI√ìN**")
            st.markdown("""
            **Diagn√≥stico:** El modelo muestra una excelente capacidad de generalizaci√≥n.
            
            **Implicaciones:**
            - Muy poca diferencia entre rendimiento de entrenamiento y validaci√≥n
            - El modelo deber√≠a funcionar bien en datos nuevos
            - Posible candidato para el modelo final
            """)
    
    with col2:
        st.metric("Gap Final", f"{final_gap:.4f}")
        st.metric("Gap M√°ximo", f"{max_gap:.4f}")
        st.metric("Tendencia", gap_trend.title())
        
        # Medidor visual del gap
        if final_gap > 0.15:
            st.markdown("üî¥üî¥üî¥üî¥üî¥")
        elif final_gap > 0.1:
            st.markdown("üü†üü†üü†üü†‚ö™")
        elif final_gap > 0.05:
            st.markdown("üü°üü°üü°‚ö™‚ö™")
        else:
            st.markdown("üü¢üü¢üü¢üü¢üü¢")
    
    # Recomendaciones espec√≠ficas basadas en el gap
    with st.expander("üí° Recomendaciones t√©cnicas", expanded=False):
        if final_gap > 0.1:
            st.markdown("""
            **Estrategias para reducir overfitting:**
            
            1. **Regularizaci√≥n:**
               - Aumentar par√°metros de regularizaci√≥n (L1, L2)
               - Usar dropout en redes neuronales
               
            2. **Datos:**
               - Aumentar el tama√±o del conjunto de datos
               - Aplicar t√©cnicas de data augmentation
               
            3. **Modelo:**
               - Reducir la complejidad del modelo
               - Usar early stopping durante el entrenamiento
               
            4. **Validaci√≥n:**
               - Implementar validaci√≥n cruzada m√°s robusta
               - Usar ensemble methods
            """)
        else:
            st.markdown("""
            **El modelo muestra buen comportamiento. Consideraciones adicionales:**
            
            1. **Optimizaci√≥n:**
               - Evaluar si se puede mejorar el rendimiento general
               - Considerar t√©cnicas de ensemble
               
            2. **Validaci√≥n:**
               - Probar en un conjunto de datos completamente independiente
               - Validar en datos de diferentes per√≠odos de tiempo
               
            3. **Monitoreo:**
               - Establecer alertas para detectar drift del modelo
               - Monitorear rendimiento en producci√≥n
            """)


def mostrar_metricas_validacion_mejoradas(metricas, cv_results_completos, tipo_problema):
    """Muestra las m√©tricas de validaci√≥n mejoradas con resultados completos de CV."""
    if not metricas and not cv_results_completos:
        st.info("üìä No hay m√©tricas de validaci√≥n disponibles")
        return
    
    st.subheader("üìà M√©tricas de Validaci√≥n Detalladas")
    
    # M√©tricas principales en columnas
    col1, col2, col3 = st.columns(3)
    
    # M√©tricas originales del modelo
    with col1:
        st.write("**M√©tricas del Modelo:**")
        for metrica, valor in metricas.items():
            if isinstance(valor, (int, float)):
                st.metric(metrica.replace('_', ' ').title(), f"{valor:.4f}")
            else:
                st.write(f"- **{metrica.replace('_', ' ').title()}:** {valor}")
    
    # Resultados de validaci√≥n cruzada
    with col2:
        if cv_results_completos:
            st.write("**Validaci√≥n Cruzada:**")
            mean_score = cv_results_completos.get('mean_score', 0)
            std_score = cv_results_completos.get('std_score', 0)
            cv_folds = cv_results_completos.get('cv_folds', 5)
            scoring_metric = cv_results_completos.get('scoring_metric', 'N/A')
            
            st.metric("Media CV", f"{mean_score:.4f}")
            st.metric("Desviaci√≥n CV", f"{std_score:.4f}")
            st.write(f"- **Folds:** {cv_folds}")
            st.write(f"- **M√©trica:** {scoring_metric.upper()}")
    
    # Estad√≠sticas adicionales
    with col3:
        if cv_results_completos:
            st.write("**Estad√≠sticas CV:**")
            min_score = cv_results_completos.get('min_score', 0)
            max_score = cv_results_completos.get('max_score', 0)
            variance = cv_results_completos.get('variance', 0)
            
            st.metric("M√≠nimo CV", f"{min_score:.4f}")
            st.metric("M√°ximo CV", f"{max_score:.4f}")
            st.metric("Varianza CV", f"{variance:.6f}")
    
    # Gr√°fico de distribuci√≥n de puntuaciones CV
    if cv_results_completos and cv_results_completos.get('cv_scores'):
        cv_scores = cv_results_completos['cv_scores']
        
        with st.expander("üìä Distribuci√≥n de Puntuaciones CV", expanded=False):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Gr√°fico de barras
                df_cv = pd.DataFrame({
                    'Fold': [f'Fold {i+1}' for i in range(len(cv_scores))],
                    'Puntuaci√≥n': cv_scores
                })
                st.bar_chart(df_cv.set_index('Fold'))
            
            with col2:
                # Estad√≠sticas resumidas
                st.write("**Resumen estad√≠stico:**")
                st.write(f"‚Ä¢ **Media:** {np.mean(cv_scores):.4f}")
                st.write(f"‚Ä¢ **Mediana:** {np.median(cv_scores):.4f}")
                st.write(f"‚Ä¢ **Desv. Std:** {np.std(cv_scores):.4f}")
                st.write(f"‚Ä¢ **Rango:** {np.max(cv_scores) - np.min(cv_scores):.4f}")
                
                # Interpretaci√≥n de consistencia
                if np.std(cv_scores) < 0.02:
                    st.success("üü¢ Muy consistente")
                elif np.std(cv_scores) < 0.05:
                    st.info("üü° Consistencia normal")
                else:
                    st.warning("üî¥ Inconsistente")

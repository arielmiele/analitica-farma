"""
M√≥dulo para la evaluaci√≥n detallada de modelos de machine learning.
"""
import json
import os
import sqlite3
from datetime import datetime
import logging
from typing import Dict, Optional, Tuple
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
import pandas as pd
import traceback

# Configuraci√≥n del logger
logger = logging.getLogger("evaluador")

def evaluar_modelo_detallado(
    id_benchmarking: int,
    nombre_modelo: str,
    id_usuario: int = 1,
    db_path: Optional[str] = None
) -> Dict:
    """
    Realiza una evaluaci√≥n detallada de un modelo espec√≠fico.
    
    Args:
        id_benchmarking: ID del benchmarking previamente ejecutado
        nombre_modelo: Nombre del modelo a evaluar en detalle
        id_usuario: ID del usuario que realiza la acci√≥n
        db_path: Ruta a la base de datos
        
    Returns:
        Dict: Resultados detallados de la evaluaci√≥n
    """
    conn = None
    try:
        # Determinar la ruta de la base de datos
        if db_path is None:
            db_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'analitica_farma.db')
        
        # Conectar a la base de datos
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Obtener los resultados del benchmarking
        cursor.execute(
            "SELECT resultados_completos FROM benchmarking_modelos WHERE id = ?", 
            (id_benchmarking,)
        )
        
        resultado = cursor.fetchone()
        if not resultado:
            return {"error": f"No se encontr√≥ el benchmarking con ID {id_benchmarking}"}
        
        # Cargar resultados
        benchmarking = json.loads(resultado['resultados_completos'])
        
        # Buscar el modelo espec√≠fico
        modelo_encontrado = None
        for modelo in benchmarking['modelos_exitosos']:
            if modelo['nombre'] == nombre_modelo:
                modelo_encontrado = modelo
                break
        
        if not modelo_encontrado:
            return {"error": f"No se encontr√≥ el modelo {nombre_modelo} en los resultados"}
        
        # Registrar en la tabla de auditor√≠a
        cursor.execute("""
            INSERT INTO auditoria (
                id_usuario, accion, descripcion, fecha
            ) VALUES (?, ?, ?, ?)
        """, (
            id_usuario,
            "EVALUACION_DETALLADA",
            f"Evaluaci√≥n detallada del modelo {nombre_modelo}",
            datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        ))
        
        # Confirmar cambios
        conn.commit()
        
        # Retornar la informaci√≥n detallada del modelo
        return {
            "modelo": modelo_encontrado,
            "tipo_problema": benchmarking['tipo_problema'],
            "variable_objetivo": benchmarking['variable_objetivo'],
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    except Exception as e:
        logger.error(f"Error al evaluar modelo detallado: {str(e)}")
        if conn:
            conn.rollback()
        return {"error": str(e)}
    finally:
        if conn:
            conn.close()

def obtener_ultimos_benchmarkings(
    limite: int = 5,
    id_usuario: Optional[int] = None,
    db_path: Optional[str] = None
) -> list:
    """
    Obtiene los √∫ltimos benchmarkings ejecutados.
    
    Args:
        limite: N√∫mero m√°ximo de resultados a devolver
        id_usuario: ID del usuario para filtrar
        db_path: Ruta a la base de datos
        
    Returns:
        list: Lista de benchmarkings
    """
    conn = None
    try:
        # Determinar la ruta de la base de datos
        if db_path is None:
            db_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'analitica_farma.db')
        
        # Conectar a la base de datos
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Construir la consulta
        query = """
            SELECT id, tipo_problema, variable_objetivo, 
                   cantidad_modelos_exitosos, cantidad_modelos_fallidos, 
                   mejor_modelo, fecha_ejecucion 
            FROM benchmarking_modelos 
            WHERE 1=1
        """
        params = []
        
        if id_usuario:
            query += " AND id_usuario = ?"
            params.append(id_usuario)
        
        query += " ORDER BY fecha_ejecucion DESC LIMIT ?"
        params.append(limite)
        
        # Ejecutar la consulta
        cursor.execute(query, params)
        
        # Obtener y devolver resultados
        resultados = []
        for row in cursor.fetchall():
            resultados.append({
                "id": row['id'],
                "tipo_problema": row['tipo_problema'],
                "variable_objetivo": row['variable_objetivo'],
                "modelos_exitosos": row['cantidad_modelos_exitosos'],
                "modelos_fallidos": row['cantidad_modelos_fallidos'],
                "mejor_modelo": row['mejor_modelo'],
                "fecha": row['fecha_ejecucion']
            })
            
        return resultados
    
    except Exception as e:
        logger.error(f"Error al obtener √∫ltimos benchmarkings: {str(e)}")
        return []
    finally:
        if conn:
            conn.close()

def generar_curvas_aprendizaje(
    id_benchmarking: int,
    nombre_modelo: str,
    db_path: Optional[str] = None
) -> Dict:
    """
    Genera curvas de aprendizaje para detectar overfitting/underfitting.
    
    Args:
        id_benchmarking: ID del benchmarking
        nombre_modelo: Nombre del modelo
        db_path: Ruta a la base de datos
        
    Returns:
        Dict: Resultados del an√°lisis de curvas de aprendizaje
    """
    conn = None
    try:
        # Determinar la ruta de la base de datos
        if db_path is None:
            db_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'analitica_farma.db')
        
        # Conectar a la base de datos
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Obtener los resultados del benchmarking
        cursor.execute(
            "SELECT resultados_completos FROM benchmarking_modelos WHERE id = ?", 
            (id_benchmarking,)
        )
        row = cursor.fetchone()
        
        if not row:
            return {
                'error': f'No se encontr√≥ el benchmarking con ID {id_benchmarking}',
                'solucion': 'Verifique que el ID sea correcto o ejecute un nuevo benchmarking'
            }
        
        # Deserializar los resultados
        resultados_benchmarking = json.loads(row['resultados_completos'])
        
        # Buscar el modelo espec√≠fico
        modelo_encontrado = None
        for modelo in resultados_benchmarking.get('modelos_exitosos', []):
            if modelo['nombre'] == nombre_modelo:
                modelo_encontrado = modelo
                break
        
        if not modelo_encontrado:
            return {
                'error': f'No se encontr√≥ el modelo {nombre_modelo} en el benchmarking',
                'solucion': 'Verifique que el nombre del modelo sea correcto'
            }
        
        # Verificar que tenemos datos de prueba
        if 'X_test' not in resultados_benchmarking or 'y_test' not in resultados_benchmarking:
            return {
                'error': 'No se encontraron datos de prueba en el benchmarking',
                'solucion': 'Los datos de prueba son necesarios para el an√°lisis. Ejecute un nuevo benchmarking'
            }
        
        # Generar an√°lisis simulado de curvas de aprendizaje
        # (Una implementaci√≥n completa requerir√≠a re-entrenar el modelo con diferentes tama√±os de datos)
        
        # Obtener m√©tricas existentes del modelo
        metricas = modelo_encontrado.get('metricas', {})
        cv_scores = modelo_encontrado.get('cv_scores', [])
        
        # Simular diagn√≥stico basado en m√©tricas disponibles
        diagnostico = generar_diagnostico_overfitting(metricas, cv_scores)
        
        resultados = {
            'modelo': nombre_modelo,
            'tipo_problema': resultados_benchmarking.get('tipo_problema'),
            'metricas_principales': metricas,
            'cv_scores': cv_scores,
            'diagnostico': diagnostico,
            'recomendaciones': generar_recomendaciones_validacion(diagnostico, metricas),
            'datos_disponibles': {
                'X_test_shape': str(np.array(resultados_benchmarking['X_test']).shape) if 'X_test' in resultados_benchmarking else 'N/A',
                'y_test_shape': str(np.array(resultados_benchmarking['y_test']).shape) if 'y_test' in resultados_benchmarking else 'N/A',
                'total_filas': resultados_benchmarking.get('total_filas', 'N/A'),
                'porcentaje_test': resultados_benchmarking.get('porcentaje_test', 'N/A')
            },
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        conn.close()
        return resultados
        
    except Exception as e:
        logger.error(f"Error generando curvas de aprendizaje: {str(e)}")
        if conn:
            conn.close()
        return {
            'error': f'Error interno: {str(e)}',
            'solucion': 'Verifique los datos y vuelva a intentar'
        }


def generar_diagnostico_overfitting(metricas: Dict, cv_scores: list) -> Dict:
    """
    Genera un diagn√≥stico de overfitting/underfitting basado en m√©tricas disponibles.
    
    Args:
        metricas: M√©tricas del modelo
        cv_scores: Puntuaciones de validaci√≥n cruzada
        
    Returns:
        Dict: Diagn√≥stico del modelo
    """
    diagnostico = {
        'overfitting': 'desconocido',
        'underfitting': 'desconocido',
        'varianza_cv': 0,
        'mensaje': '',
        'nivel_confianza': 'bajo'
    }
    
    if cv_scores:
        cv_mean = np.mean(cv_scores)
        cv_std = np.std(cv_scores)
        diagnostico['varianza_cv'] = cv_std
        
        # An√°lisis de varianza en CV
        if cv_std > 0.1:  # Alta varianza
            diagnostico['overfitting'] = 'posible'
            diagnostico['mensaje'] = f'Alta varianza en CV (œÉ={cv_std:.3f}). Posible overfitting.'
        elif cv_std < 0.03:  # Baja varianza
            diagnostico['overfitting'] = 'improbable'
            diagnostico['mensaje'] = f'Baja varianza en CV (œÉ={cv_std:.3f}). Modelo estable.'
        else:
            diagnostico['overfitting'] = 'normal'
            diagnostico['mensaje'] = f'Varianza en CV normal (œÉ={cv_std:.3f}).'
        
        # An√°lisis de rendimiento general
        if cv_mean < 0.6:  # Bajo rendimiento
            diagnostico['underfitting'] = 'posible'
            diagnostico['mensaje'] += f' Rendimiento bajo (Œº={cv_mean:.3f}). Posible underfitting.'
        else:
            diagnostico['underfitting'] = 'improbable'
        
        diagnostico['nivel_confianza'] = 'medio'
    
    return diagnostico


def generar_recomendaciones_validacion(diagnostico: Dict, metricas: Dict) -> list:
    """
    Genera recomendaciones basadas en el diagn√≥stico de validaci√≥n.
    
    Args:
        diagnostico: Diagn√≥stico del modelo
        metricas: M√©tricas del modelo
        
    Returns:
        list: Lista de recomendaciones
    """
    recomendaciones = []
    
    if diagnostico['overfitting'] == 'posible':
        recomendaciones.extend([
            "üîÑ Considere usar regularizaci√≥n (L1/L2)",
            "üìä Aumente el tama√±o del dataset de entrenamiento",
            "üå≥ Reduzca la complejidad del modelo",
            "‚úÇÔ∏è Aplique t√©cnicas de feature selection"
        ])
    
    if diagnostico['underfitting'] == 'posible':
        recomendaciones.extend([
            "üîß Aumente la complejidad del modelo",
            "üéØ Agregue m√°s caracter√≠sticas relevantes",
            "‚öôÔ∏è Ajuste los hiperpar√°metros",
            "üîç Verifique la calidad de los datos"
        ])
    
    if diagnostico['varianza_cv'] > 0.1:
        recomendaciones.append("üé≤ Considere usar ensemble methods para reducir varianza")
    
    if not recomendaciones:
        recomendaciones.append("‚úÖ El modelo muestra un comportamiento balanceado")
    
    return recomendaciones

def cargar_benchmarking_seleccionado(
    benchmarking_id: int,
    db_path: Optional[str] = None
) -> Dict:
    """
    Carga un benchmarking espec√≠fico desde la base de datos.
    
    Args:
        benchmarking_id: ID del benchmarking a cargar
        db_path: Ruta a la base de datos (opcional)
        
    Returns:
        Dict: Datos completos del benchmarking o diccionario con error
    """
    conn = None
    try:
        # Determinar la ruta de la base de datos
        if db_path is None:
            db_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'analitica_farma.db')
        
        # Conectar a la base de datos
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Obtener los resultados del benchmarking
        cursor.execute(
            "SELECT resultados_completos FROM benchmarking_modelos WHERE id = ?", 
            (benchmarking_id,)
        )
        
        resultado = cursor.fetchone()
        if not resultado:
            return {"error": f"No se encontr√≥ el benchmarking con ID {benchmarking_id}"}
        
        # Cargar resultados JSON
        try:
            benchmarking = json.loads(resultado['resultados_completos'])
            return benchmarking
        except json.JSONDecodeError as e:
            return {"error": f"Error al decodificar JSON del benchmarking: {str(e)}"}
            
    except Exception as e:
        error_detalle = traceback.format_exc()
        logger.error(f"Error al cargar benchmarking: {str(e)}\n{error_detalle}")
        return {"error": str(e)}
    finally:
        if conn:
            conn.close()

def obtener_modelo_desde_benchmarking(
    resultados_benchmarking: Dict,
    nombre_modelo: Optional[str] = None
) -> Tuple[Dict, bool]:
    """
    Obtiene un modelo espec√≠fico desde los resultados de un benchmarking.
    Si no se especifica nombre_modelo, retorna el mejor modelo.
    
    Args:
        resultados_benchmarking: Diccionario con resultados del benchmarking
        nombre_modelo: Nombre del modelo a obtener (opcional)
        
    Returns:
        Tuple[Dict, bool]: (Modelo encontrado, Es el mejor modelo)
    """
    try:
        # Verificar si hay modelos exitosos
        if not resultados_benchmarking.get('modelos_exitosos'):
            return {"error": "No hay modelos exitosos en el benchmarking"}, False
        
        # Si no se especifica modelo, tomar el mejor
        if nombre_modelo is None:
            # Si hay un mejor modelo definido, usarlo
            if 'mejor_modelo' in resultados_benchmarking and resultados_benchmarking['mejor_modelo']:
                modelo = resultados_benchmarking['mejor_modelo']
                return modelo, True
            # Si no, tomar el primer modelo exitoso
            else:
                return resultados_benchmarking['modelos_exitosos'][0], False
        
        # Buscar el modelo por nombre
        for modelo in resultados_benchmarking['modelos_exitosos']:
            if modelo['nombre'] == nombre_modelo:
                # Determinar si es el mejor modelo
                es_mejor = False
                if 'mejor_modelo' in resultados_benchmarking:
                    mejor = resultados_benchmarking['mejor_modelo']
                    if mejor and 'nombre' in mejor and mejor['nombre'] == nombre_modelo:
                        es_mejor = True
                
                return modelo, es_mejor
        
        # Si no se encontr√≥ el modelo
        return {"error": f"No se encontr√≥ el modelo {nombre_modelo}"}, False
        
    except Exception as e:
        logger.error(f"Error al obtener modelo desde benchmarking: {str(e)}")
        return {"error": str(e)}, False
def generar_visualizaciones_clasificacion(
    modelo: Dict,
    resultados_benchmarking: Dict,
    tipo_visualizacion: str
) -> Figure:
    """
    Genera visualizaciones espec√≠ficas para problemas de clasificaci√≥n.
    
    Args:
        modelo: Diccionario con informaci√≥n del modelo
        resultados_benchmarking: Resultados completos del benchmarking
        tipo_visualizacion: Tipo de visualizaci√≥n a generar
        
    Returns:
        plt.Figure: Figura con la visualizaci√≥n generada
    """
    from src.modelos.visualizador import (
        generar_matriz_confusion, 
        generar_curva_roc, 
        generar_curva_precision_recall
    )
    
    try:
        # Convertir datos serializados a numpy arrays si es necesario
        X_test = np.array(resultados_benchmarking['X_test']) if isinstance(resultados_benchmarking['X_test'], list) else resultados_benchmarking['X_test']
        y_test = np.array(resultados_benchmarking['y_test']) if isinstance(resultados_benchmarking['y_test'], list) else resultados_benchmarking['y_test']
        
        # Determinar si tenemos el modelo o solo m√©tricas
        if 'modelo_objeto' in modelo:
            # Obtener predicciones
            y_pred = modelo['modelo_objeto'].predict(X_test)
            
            # Obtener probabilidades (para curvas ROC)
            try:
                y_prob = modelo['modelo_objeto'].predict_proba(X_test)
            except Exception as e:
                # Si el modelo no soporta predict_proba
                logger.warning(f"El modelo no soporta predict_proba: {str(e)}")
                y_prob = np.zeros((len(y_test), len(np.unique(y_test))))
        else:
            # Si no hay modelo, usar resultados pre-calculados o dummy data
            y_pred = y_test  # Fallback
            y_prob = np.zeros((len(y_test), len(np.unique(y_test))))
        
        # Obtener clases
        clases = np.unique(y_test)
        
        # Generar visualizaci√≥n espec√≠fica
        if tipo_visualizacion == "matriz_confusion":
            return generar_matriz_confusion(
                y_test, 
                y_pred, 
                clases=[str(c) for c in clases],
                normalizar=None,
                titulo=f"Matriz de Confusi√≥n - {modelo['nombre']}"
            )
        elif tipo_visualizacion == "curva_roc":
            return generar_curva_roc(
                y_test, 
                y_prob, 
                clases=[str(c) for c in clases],
                titulo=f"Curva ROC - {modelo['nombre']}"
            )
        elif tipo_visualizacion == "precision_recall":
            return generar_curva_precision_recall(
                y_test, 
                y_prob, 
                clases=[str(c) for c in clases],
                titulo=f"Curva Precision-Recall - {modelo['nombre']}"
            )
        elif tipo_visualizacion == "comparar_modelos":
            # Implementar comparaci√≥n entre modelos
            # Por ahora, simplemente devolvemos otra visualizaci√≥n
            return generar_curva_roc(
                y_test, 
                y_prob, 
                clases=[str(c) for c in clases],
                titulo=f"Curva ROC - {modelo['nombre']}"
            )
        else:
            # Crear un gr√°fico gen√©rico si el tipo de visualizaci√≥n no es reconocido
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, f"Visualizaci√≥n no disponible: {tipo_visualizacion}", 
                   ha='center', va='center', transform=ax.transAxes)
            return fig
            
    except Exception as e:
        # Crear un gr√°fico de error
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"Error al generar visualizaci√≥n: {str(e)}", 
               ha='center', va='center', transform=ax.transAxes)
        return fig
def generar_visualizaciones_regresion(
    modelo: Dict,
    resultados_benchmarking: Dict,
    tipo_visualizacion: str
) -> Figure:
    """
    Genera visualizaciones espec√≠ficas para problemas de regresi√≥n.
    
    Args:
        modelo: Diccionario con informaci√≥n del modelo
        resultados_benchmarking: Resultados completos del benchmarking
        tipo_visualizacion: Tipo de visualizaci√≥n a generar
        
    Returns:
        plt.Figure: Figura con la visualizaci√≥n generada
    """
    from src.modelos.visualizador import (
        generar_grafico_residuos,
        comparar_distribuciones
    )
    
    try:
        # Convertir datos serializados a numpy arrays si es necesario
        X_test = np.array(resultados_benchmarking['X_test']) if isinstance(resultados_benchmarking['X_test'], list) else resultados_benchmarking['X_test']
        y_test = np.array(resultados_benchmarking['y_test']) if isinstance(resultados_benchmarking['y_test'], list) else resultados_benchmarking['y_test']
        
        # Determinar si tenemos el modelo o solo m√©tricas
        if 'modelo_objeto' in modelo:
            # Obtener predicciones
            y_pred = modelo['modelo_objeto'].predict(X_test)
        else:
            # Si no hay modelo, usar resultados pre-calculados o dummy data
            y_pred = y_test * 0.9 + np.random.normal(0, 0.1, len(y_test))  # Fallback con algo de ruido
        
        # Generar visualizaci√≥n espec√≠fica
        if tipo_visualizacion == "residuos":
            return generar_grafico_residuos(
                y_test, 
                y_pred, 
                titulo=f"Gr√°fico de Residuos - {modelo['nombre']}"
            )
        elif tipo_visualizacion == "valores_reales_vs_predichos":
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.scatter(y_test, y_pred, alpha=0.6)
            ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
            ax.set_xlabel('Valores Reales')
            ax.set_ylabel('Valores Predichos')
            ax.set_title(f'Valores Reales vs Predichos - {modelo["nombre"]}')
            ax.grid(True, linestyle='--', alpha=0.7)
            return fig
        elif tipo_visualizacion == "distribucion":
            return comparar_distribuciones(
                y_test,
                y_pred,
                titulo=f"Comparaci√≥n de Distribuciones - {modelo['nombre']}"
            )
        else:
            # Crear un gr√°fico gen√©rico si el tipo de visualizaci√≥n no es reconocido
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, f"Visualizaci√≥n no disponible: {tipo_visualizacion}", 
                   ha='center', va='center', transform=ax.transAxes)
            return fig
            
    except Exception as e:
        # Crear un gr√°fico de error
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"Error al generar visualizaci√≥n: {str(e)}", 
               ha='center', va='center', transform=ax.transAxes)
        logger.error(f"Error en visualizaci√≥n de regresi√≥n: {str(e)}")
        return fig

def diagnosticar_visualizaciones(resultados_benchmarking: Dict, modelo: Dict) -> Dict:
    """
    Diagnostica si se cumplen los requisitos para generar visualizaciones avanzadas
    y proporciona recomendaciones si hay problemas.
    
    Args:
        resultados_benchmarking: Resultados completos del benchmarking
        modelo: Diccionario con informaci√≥n del modelo seleccionado
        
    Returns:
        Dict: Diccionario con estado de cada requisito y recomendaciones
    """
    diagnostico = {
        "requisitos": {},
        "recomendaciones": [],
        "puede_visualizar": True
    }
    
    # Verificar disponibilidad de datos de prueba
    if 'X_test' not in resultados_benchmarking or 'y_test' not in resultados_benchmarking:
        diagnostico["requisitos"]["datos_prueba"] = False
        diagnostico["recomendaciones"].append(
            "No se encontraron datos de prueba (X_test, y_test). Ejecute un nuevo benchmarking para generar visualizaciones avanzadas."
        )
        diagnostico["puede_visualizar"] = False
    else:
        diagnostico["requisitos"]["datos_prueba"] = True
    
    # Verificar disponibilidad del objeto modelo
    if 'modelo_objeto' not in modelo:
        diagnostico["requisitos"]["modelo_objeto"] = False
        diagnostico["recomendaciones"].append(
            "El objeto del modelo no est√° disponible. Esto puede ocurrir cuando los modelos no se deserializan correctamente. "
            "Si ha cargado datos nuevos, ejecute un nuevo benchmarking para garantizar que los modelos est√©n disponibles."
        )
        diagnostico["puede_visualizar"] = False
    else:
        diagnostico["requisitos"]["modelo_objeto"] = True
    
    # Verificar que el modelo sea compatible con predict_proba (para clasificaci√≥n)
    if resultados_benchmarking.get('tipo_problema') == 'clasificacion' and diagnostico["requisitos"].get("modelo_objeto", False):
        try:
            # Comprobar si el modelo tiene el m√©todo predict_proba
            if hasattr(modelo['modelo_objeto'], 'predict_proba'):
                diagnostico["requisitos"]["predict_proba"] = True
            else:
                diagnostico["requisitos"]["predict_proba"] = False
                diagnostico["recomendaciones"].append(
                    f"El modelo {modelo['nombre']} no admite el c√°lculo de probabilidades (predict_proba). "
                    "Algunas visualizaciones como curvas ROC pueden no estar disponibles o ser limitadas."
                )
        except Exception as e:
            diagnostico["requisitos"]["predict_proba"] = False
            diagnostico["recomendaciones"].append(
                f"Error al verificar compatibilidad con predict_proba: {str(e)}"
            )
    
    # Verificar que haya suficientes m√©tricas calculadas
    if not modelo.get('metricas'):
        diagnostico["requisitos"]["metricas"] = False
        diagnostico["recomendaciones"].append(
            "No se encontraron m√©tricas para el modelo. Algunas visualizaciones pueden no estar disponibles."
        )
    else:
        diagnostico["requisitos"]["metricas"] = True
    
    # Verificar tipo de problema compatible
    if resultados_benchmarking.get('tipo_problema') not in ['clasificacion', 'regresion']:
        diagnostico["requisitos"]["tipo_problema"] = False
        diagnostico["recomendaciones"].append(
            f"El tipo de problema '{resultados_benchmarking.get('tipo_problema')}' no es compatible con las visualizaciones avanzadas disponibles."
        )
        diagnostico["puede_visualizar"] = False
    else:
        diagnostico["requisitos"]["tipo_problema"] = True
    
    return diagnostico

def generar_tabla_metricas(modelo: Dict, tipo_problema: str) -> pd.DataFrame:
    """
    Genera una tabla de pandas con las m√©tricas del modelo.
    
    Args:
        modelo: Diccionario con informaci√≥n del modelo
        tipo_problema: Tipo de problema (clasificaci√≥n o regresi√≥n)
        
    Returns:
        pd.DataFrame: DataFrame con las m√©tricas
    """
    try:
        # Obtener m√©tricas del modelo
        metricas = modelo.get('metricas', {})
        
        # Crear dataframe seg√∫n tipo de problema
        if tipo_problema == 'clasificacion':
            metricas_clave = ['accuracy', 'precision', 'recall', 'f1', 'auc']
            nombres_metricas = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
        else:  # regresi√≥n
            metricas_clave = ['r2', 'mae', 'mse', 'rmse', 'mape']
            nombres_metricas = ['R¬≤', 'MAE', 'MSE', 'RMSE', 'MAPE']
        
        # Filtrar m√©tricas disponibles
        valores = []
        nombres_filtrados = []
        
        for clave, nombre in zip(metricas_clave, nombres_metricas):
            if clave in metricas:
                valores.append(metricas[clave])
                nombres_filtrados.append(nombre)
        
        # Crear dataframe
        df_metricas = pd.DataFrame({
            'M√©trica': nombres_filtrados,
            'Valor': valores
        })
        
        return df_metricas
    
    except Exception as e:
        logger.error(f"Error al generar tabla de m√©tricas: {str(e)}")
        # Devolver un dataframe vac√≠o en caso de error
        return pd.DataFrame({'M√©trica': [], 'Valor': []})
def comparar_metricas_regresion(modelos_dict, X_test, y_test):
    """
    Compara m√©tricas de regresi√≥n para m√∫ltiples modelos.
    
    Args:
        modelos_dict: Diccionario con nombre -> {'modelo': modelo_objeto}
        X_test: Datos de prueba
        y_test: Etiquetas verdaderas de prueba
        
    Returns:
        pd.DataFrame: DataFrame con m√©tricas comparativas
    """
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    
    try:
        # Crear DataFrame para comparar m√©tricas de los modelos
        metricas_comp = pd.DataFrame()
        
        for nombre, info in modelos_dict.items():
            y_pred = info['modelo'].predict(X_test)
            
            r2 = r2_score(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test, y_pred)
            
            # A√±adir m√©tricas al DataFrame de comparaci√≥n
            metricas_comp[nombre] = [r2, mse, rmse, mae]
        
        # Establecer nombres de filas para las m√©tricas
        metricas_comp.index = pd.Index(['R¬≤', 'MSE', 'RMSE', 'MAE'])
        
        return metricas_comp
        
    except Exception as e:
        logger.error(f"Error al comparar m√©tricas de regresi√≥n: {str(e)}")
        return pd.DataFrame()

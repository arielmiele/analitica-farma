"""
P√°gina para evaluar y comparar en detalle los modelos entrenados.
Incluye visualizaciones avanzadas como matrices de confusi√≥n, curvas ROC, gr√°ficos de residuos
y otras herramientas para an√°lisis detallado del rendimiento de modelos.
"""
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import io

# Importar m√≥dulos propios
from src.modelos.evaluador import (
    obtener_ultimos_benchmarkings, 
    diagnosticar_visualizaciones,
    comparar_metricas_regresion,
    calcular_matriz_confusion_detallada,
    calcular_curvas_roc_completas,
    generar_datos_grafico_comparacion_regresion
)
from src.modelos.visualizador import (
    generar_matriz_confusion, 
    generar_curva_roc, 
    generar_curva_precision_recall,
    generar_grafico_residuos, 
    comparar_distribuciones,
    comparar_modelos_roc,
    generar_interpretacion_matriz_confusion,
    generar_interpretacion_curva_roc,
    generar_interpretacion_residuos
)
from src.modelos.modelo_serializer import deserializar_modelos_benchmarking
from src.modelos.diagnostico_modelo import diagnosticar_objetos_modelo
from src.state.session_manager import SessionManager
from src.audit.logger import log_audit

# Inicializar gestor de sesi√≥n
session = SessionManager()

def main():
    st.title("üìä Evaluaci√≥n Detallada de Modelos")
    
    # Verificar si hay resultados de benchmarking
    resultados_benchmarking = session.obtener_estado("resultados_benchmarking")
    
    # Si hay resultados, verificar si los modelos necesitan ser deserializados
    if resultados_benchmarking:
        # Comprobar si hay modelos que tienen 'modelo_serializado' pero no 'modelo_objeto'
        necesita_deserializacion = False
        
        if resultados_benchmarking.get('modelos_exitosos'):
            for modelo in resultados_benchmarking['modelos_exitosos']:
                if 'modelo_serializado' in modelo and 'modelo_objeto' not in modelo:
                    necesita_deserializacion = True
                    break
        
        # Si se necesita deserializaci√≥n, aplicarla
        if necesita_deserializacion:
            resultados_benchmarking = deserializar_modelos_benchmarking(
                resultados_benchmarking,
                usuario=session.obtener_estado("usuario_id", "sistema"),
                id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
            )
            session.guardar_estado("resultados_benchmarking", resultados_benchmarking)
            st.success("‚úÖ Modelos deserializados autom√°ticamente")
        
        # Mostrar diagn√≥stico de objetos modelo (debugging)
        with st.expander("üìã Diagn√≥stico de modelos", expanded=False):
            diagnosticar_objetos_modelo(
                resultados_benchmarking,
                usuario=session.obtener_estado("usuario_id", "sistema"),
                id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
            )
            st.info("Si los objetos modelo no est√°n disponibles, intente ejecutar un nuevo benchmarking o cargar uno existente.")
    
    if not resultados_benchmarking:
        st.warning("‚ö†Ô∏è No hay resultados de benchmarking disponibles.")
        st.info("üëà Vaya a la secci√≥n 'Entrenar Modelos' para ejecutar un benchmarking primero.")
        
        # Mostrar benchmarkings anteriores si existen
        try:
            benchmarkings_previos = obtener_ultimos_benchmarkings(
                limite=5,
                id_usuario=session.obtener_estado("usuario_id", "sistema"),
                id_sesion=session.obtener_estado("id_sesion", "sin_sesion"),
                usuario=session.obtener_estado("usuario_id", "sistema")
            )
            if benchmarkings_previos:
                st.subheader("üìú Benchmarkings anteriores")
                
                # Crear tabla para mostrar los benchmarkings
                df_benchmarkings = pd.DataFrame(benchmarkings_previos)
                df_benchmarkings.columns = [
                    "ID", "Tipo de problema", "Variable objetivo", 
                    "Modelos exitosos", "Modelos fallidos", 
                    "Mejor modelo", "Fecha"
                ]
                
                st.dataframe(df_benchmarkings)
                  # Permitir cargar un benchmarking anterior
                selected_id = st.selectbox(
                    "Seleccione un benchmarking para cargar:",
                    options=df_benchmarkings["ID"].tolist()
                )
                
                if st.button("Cargar benchmarking seleccionado"):
                    try:
                        # Aqu√≠ implementamos la carga del benchmarking seleccionado
                        from src.modelos.entrenador import obtener_benchmarking_por_id
                        
                        # Registrar en el log
                        log_audit(
                            session.obtener_estado("usuario_id", "sistema"),
                            "CARGAR_BENCHMARKING",
                            "Evaluar_Modelos",
                            selected_id,
                            f"Cargando benchmarking ID: {selected_id}",
                            id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                        )
                        
                        # Verificar que selected_id no sea None antes de convertirlo
                        if selected_id is not None:
                            # Obtener el benchmarking y deserializar los modelos
                            benchmarking = obtener_benchmarking_por_id(int(selected_id))
                            
                            # Deserializar expl√≠citamente para asegurar que los objetos modelo est√©n disponibles
                            if benchmarking:
                                benchmarking = deserializar_modelos_benchmarking(
                                    benchmarking,
                                    usuario=session.obtener_estado("usuario_id", "sistema"),
                                    id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                                )
                                st.success("‚úÖ Modelos deserializados correctamente")
                        else:
                            st.error("‚ùå ID de benchmarking no v√°lido.")
                            benchmarking = None
                        
                        if benchmarking:
                            # Guardar en sesi√≥n
                            session.guardar_estado("resultados_benchmarking", benchmarking)
                            st.success(f"‚úÖ Benchmarking ID {selected_id} cargado correctamente.")
                            
                            # Verificar si los objetos modelo est√°n disponibles
                            diagnosticar_objetos_modelo(
                                benchmarking,
                                usuario=session.obtener_estado("usuario_id", "sistema"),
                                id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                            )
                            
                            st.rerun()
                        else:
                            st.error("‚ùå No se pudo cargar el benchmarking seleccionado.")
                    except Exception as error:
                        st.error(f"‚ùå Error al cargar benchmarking: {str(error)}")
                        log_audit(
                            session.obtener_estado("usuario_id", "sistema"),
                            "ERROR_CARGAR_BENCHMARKING",
                            "Evaluar_Modelos",
                            selected_id,
                            f"Error al cargar benchmarking ID {selected_id}: {str(error)}",
                            id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                        )
            else:
                st.info("No hay benchmarkings previos disponibles.")
        except Exception as e:
            st.error(f"Error al cargar benchmarkings anteriores: {str(e)}")
        
        return
    
    # Secci√≥n de explicaci√≥n
    with st.expander("‚ÑπÔ∏è Acerca de esta p√°gina", expanded=False):
        st.markdown("""
        ### Evaluaci√≥n Detallada de Modelos
        
        Esta p√°gina permite analizar en profundidad los modelos entrenados en el benchmarking.
        
        **Funcionalidades:**
        - Selecci√≥n del modelo a evaluar
        - Visualizaci√≥n detallada de m√©tricas
        - An√°lisis de validaci√≥n cruzada
        - Comparaci√≥n con el modelo de referencia
        - **Visualizaciones avanzadas:**
          - Matrices de confusi√≥n
          - Curvas ROC y Precision-Recall
          - Gr√°ficos de residuos y distribuciones
          - Comparativas entre modelos
        
        Seleccione un modelo de la lista para ver su evaluaci√≥n detallada.
        """)
    
    # Selecci√≥n de modelo a evaluar
    if resultados_benchmarking.get('modelos_exitosos'):
        # Obtener nombres de modelos exitosos
        nombres_modelos = [modelo['nombre'] for modelo in resultados_benchmarking['modelos_exitosos']]
        
        # Determinar el modelo a mostrar (por defecto, el mejor)
        modelo_seleccionado = st.selectbox(
            "Seleccione un modelo para evaluar:",
            options=nombres_modelos,
            index=0  # Por defecto, el mejor modelo (ya est√°n ordenados)
        )
        
        # Encontrar el modelo seleccionado en los resultados
        modelo = None
        for m in resultados_benchmarking['modelos_exitosos']:
            if m['nombre'] == modelo_seleccionado:
                modelo = m
                break
        
        if modelo:
            # Mostrar informaci√≥n general del modelo
            st.subheader(f"üìù Informaci√≥n general: {modelo['nombre']}")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**Tipo de problema:** {resultados_benchmarking['tipo_problema'].capitalize()}")
                st.write(f"**Variable objetivo:** {resultados_benchmarking['variable_objetivo']}")
                
                # Determinar si es el mejor modelo
                es_mejor = modelo == resultados_benchmarking['mejor_modelo']
                if es_mejor:
                    st.success("‚úÖ Este es el mejor modelo seg√∫n la evaluaci√≥n.")
            
            with col2:
                st.write(f"**Tiempo de entrenamiento:** {modelo['tiempo_entrenamiento']:.4f} segundos")
                st.write(f"**Fecha de evaluaci√≥n:** {resultados_benchmarking['timestamp']}")
            
            # Mostrar m√©tricas detalladas
            st.subheader("üìä M√©tricas de rendimiento")
            
            # Dise√±o seg√∫n tipo de problema
            if resultados_benchmarking['tipo_problema'] == 'clasificacion':
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Accuracy", f"{modelo['metricas']['accuracy']:.4f}")
                
                with col2:
                    st.metric("Precision", f"{modelo['metricas']['precision']:.4f}")
                
                with col3:
                    st.metric("Recall", f"{modelo['metricas']['recall']:.4f}")
                
                with col4:
                    st.metric("F1-Score", f"{modelo['metricas']['f1']:.4f}")
                
                # Validaci√≥n cruzada
                st.subheader("üîÑ Validaci√≥n cruzada")
                
                # Obtener m√©tricas desde el objeto modelo
                metricas = modelo.get('metricas', {})
                cv_score = metricas.get('cv_score_media', 0)
                cv_std = metricas.get('cv_score_std', 0)
                
                st.write(f"**Score CV (promedio):** {cv_score:.4f}")
                st.write(f"**Desviaci√≥n est√°ndar CV:** {cv_std:.4f}")
                
                # Gr√°fico de barras para m√©tricas de clasificaci√≥n
                fig, ax = plt.subplots(figsize=(10, 6))
                
                metricas = ['accuracy', 'precision', 'recall', 'f1']
                valores = [modelo['metricas'][m] for m in metricas]
                # Asignar colores seg√∫n el valor (m√°s alto = mejor)
                colores = ['green', 'lightgreen', 'orange', 'red']
                
                barras = ax.bar(metricas, valores, color=colores)
                
                # A√±adir etiquetas con valores
                for i, barra in enumerate(barras):
                    ax.text(
                        barra.get_x() + barra.get_width()/2,
                        barra.get_height() + 0.01,
                        f'{valores[i]:.4f}',
                        ha='center'
                    )
                
                # Configurar gr√°fico
                plt.ylim(0, 1.1)  # M√©tricas de clasificaci√≥n van de 0 a 1
                plt.title('M√©tricas de clasificaci√≥n')
                plt.ylabel('Valor')
                plt.grid(axis='y', linestyle='--', alpha=0.7)
                
                st.pyplot(fig)                
                
                # VISUALIZACIONES AVANZADAS PARA CLASIFICACI√ìN
                st.subheader("üìà Visualizaciones avanzadas")
                
                # Realizar diagn√≥stico de requisitos para visualizaciones
                diagnostico = diagnosticar_visualizaciones(resultados_benchmarking, modelo)
                
                # Mostrar estado de diagn√≥stico
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # Mostrar estado general
                    if diagnostico["puede_visualizar"]:
                        st.success("‚úÖ Todos los requisitos para visualizaciones avanzadas est√°n disponibles")
                    else:
                        st.warning("‚ö†Ô∏è Hay limitaciones para las visualizaciones avanzadas")
                
                with col2:
                    # Mostrar detalles de los requisitos
                    requisitos = diagnostico["requisitos"]
                    
                    # Datos de prueba
                    if requisitos.get("datos_prueba", False):
                        st.write("‚úÖ Datos de prueba disponibles")
                    else:
                        st.write("‚ùå Datos de prueba no disponibles")
                    
                    # Objeto del modelo
                    if requisitos.get("modelo_objeto", False):
                        st.write("‚úÖ Objeto del modelo disponible")
                    else:
                        st.write("‚ùå Objeto del modelo no disponible")
                    
                    # M√©tricas
                    if requisitos.get("metricas", False):
                        st.write("‚úÖ M√©tricas completas disponibles")
                    else:
                        st.write("‚ùå M√©tricas incompletas")
                
                # Mostrar recomendaciones si hay problemas
                if diagnostico["recomendaciones"]:
                    with st.expander("üìã Recomendaciones para mejorar visualizaciones", expanded=True):
                        for recomendacion in diagnostico["recomendaciones"]:
                            st.info(recomendacion)
                
                # Comprobar si tenemos los datos para las visualizaciones
                if 'X_test' in resultados_benchmarking and 'y_test' in resultados_benchmarking:
                    try:
                        # Convertir datos serializados de vuelta a numpy arrays si es necesario
                        X_test = np.array(resultados_benchmarking['X_test']) if isinstance(resultados_benchmarking['X_test'], list) else resultados_benchmarking['X_test']
                        y_test = np.array(resultados_benchmarking['y_test']) if isinstance(resultados_benchmarking['y_test'], list) else resultados_benchmarking['y_test']
                        
                        # Obtener predicciones y probabilidades si est√° disponible el modelo
                        if 'modelo_objeto' in modelo:
                            # Obtener predicciones
                            y_pred = modelo['modelo_objeto'].predict(X_test)
                            
                            # Obtener probabilidades (para curvas ROC)
                            try:
                                y_prob = modelo['modelo_objeto'].predict_proba(X_test)
                            except Exception as e:
                                # Si el modelo no soporta predict_proba
                                st.warning(f"El modelo no soporta predict_proba: {str(e)}")
                                y_prob = np.zeros((len(y_test), len(np.unique(y_test))))
                                
                            # Obtener clases
                            clases = np.unique(y_test)
                        else:
                            # Si no hay modelo, usar resultados pre-calculados o mostrar mensaje
                            st.warning("El modelo no est√° disponible para generar predicciones en tiempo real.")
                            # Usar dummy data para evitar errores
                            y_pred = y_test  # Fallback
                            y_prob = np.zeros((len(y_test), len(np.unique(y_test))))
                            clases = np.unique(y_test)
                        
                        # Mostrar visualizaciones simplificadas basadas en m√©tricas disponibles
                        st.info("Mostrando visualizaciones basadas en las m√©tricas calculadas durante el entrenamiento.")
                        
                        # Ejemplo: Mostrar gr√°fico de barras de m√©tricas
                        fig, ax = plt.subplots(figsize=(10, 6))
                        metricas = modelo['metricas']
                        nombres = list(metricas.keys())
                        valores = list(metricas.values())
                        
                        # Filtrar solo m√©tricas num√©ricas relevantes (no std, etc.)
                        metricas_a_mostrar = ['accuracy', 'precision', 'recall', 'f1']
                        indices = [i for i, nombre in enumerate(nombres) if nombre in metricas_a_mostrar]
                        nombres_filtrados = [nombres[i] for i in indices]
                        valores_filtrados = [valores[i] for i in indices]
                        
                        ax.bar(nombres_filtrados, valores_filtrados, color='skyblue')
                        ax.set_ylim(0, 1)
                        ax.set_title('M√©tricas principales')
                        ax.set_ylabel('Valor')
                        ax.grid(axis='y', linestyle='--', alpha=0.7)
                        
                        # A√±adir etiquetas de valor
                        for i, v in enumerate(valores_filtrados):
                            ax.text(i, v + 0.02, f'{v:.3f}', ha='center')
                        
                        st.pyplot(fig)
                        
                        # Indicar que se necesita un nuevo benchmarking para visualizaciones completas
                        st.warning("""
                        Para visualizaciones m√°s avanzadas como matrices de confusi√≥n, curvas ROC y gr√°ficos detallados,
                        se recomienda ejecutar un nuevo benchmarking desde la secci√≥n de Entrenar Modelos.
                        """)
                        
                    except Exception as e:
                        st.error(f"Error al generar visualizaciones: {str(e)}")
                        log_audit(
                            session.obtener_estado("usuario_id", "sistema"),
                            "ERROR_VISUALIZACION",
                            "Evaluar_Modelos",
                            modelo['nombre'] if modelo else "N/A",
                            f"Error al generar visualizaciones para {modelo['nombre'] if modelo else 'N/A'}: {str(e)}",
                            id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                        )
                        return
                    
                    # Pesta√±as para diferentes visualizaciones
                    tabs = st.tabs([
                        "Matriz de Confusi√≥n", 
                        "Curva ROC", 
                        "Precision-Recall", 
                        "Comparar Modelos"
                    ])
                    
                    # TAB 1: Matriz de Confusi√≥n
                    with tabs[0]:
                            col1, col2 = st.columns([2, 1])
                            
                            with col1:
                                # Generar matriz de confusi√≥n
                                opciones_norm = {
                                    "Sin normalizar": None,
                                    "Normalizar por filas": "true",
                                    "Normalizar por columnas": "pred",
                                    "Normalizar por total": "all"
                                }
                                
                                tipo_norm = st.selectbox(
                                    "Tipo de normalizaci√≥n:",
                                    options=list(opciones_norm.keys()),
                                    index=0
                                )
                                
                                # Generar y mostrar matriz de confusi√≥n
                                matriz_confusion_fig = generar_matriz_confusion(
                                    y_test, 
                                    y_pred, 
                                    clases=[str(c) for c in clases],  # Convertir a lista de strings
                                    normalizar=opciones_norm[tipo_norm],
                                    titulo=f"Matriz de Confusi√≥n - {modelo['nombre']}",
                                    id_sesion=session.obtener_estado("id_sesion", "sin_sesion"),
                                    usuario=session.obtener_estado("usuario_id", "sistema")
                                )
                                
                                st.pyplot(matriz_confusion_fig)
                                
                                # Opci√≥n para descargar
                                buf = io.BytesIO()
                                matriz_confusion_fig.savefig(buf, format="png", dpi=120)
                                buf.seek(0)
                                st.download_button(
                                    label="Descargar matriz de confusi√≥n",
                                    data=buf,
                                    file_name=f"matriz_confusion_{modelo['nombre']}.png",
                                    mime="image/png"
                                )
                            
                            with col2:
                                # Calcular matriz para interpretaci√≥n usando funci√≥n del modelo
                                cm = calcular_matriz_confusion_detallada(y_test, y_pred, normalize=opciones_norm[tipo_norm])
                                interpretacion = generar_interpretacion_matriz_confusion(cm, [str(c) for c in clases])
                                st.markdown(interpretacion)
                        
                    # TAB 2: Curva ROC
                    with tabs[1]:
                        col1, col2 = st.columns([2, 1])
                        
                        with col1:
                            # Determinar si es multiclase
                            es_multiclase = len(np.unique(y_test)) > 2
                              # Generar y mostrar curva ROC
                            roc_fig = generar_curva_roc(
                                y_test, 
                                y_prob, 
                                clases=[str(c) for c in clases],  # Convertir a lista de strings
                                multi_clase=es_multiclase,
                                titulo=f"Curva ROC - {modelo['nombre']}",
                                id_sesion=session.obtener_estado("id_sesion", "sin_sesion"),
                                usuario=session.obtener_estado("usuario_id", "sistema")
                            )
                                
                            st.pyplot(roc_fig)
                                
                            # Opci√≥n para descargar
                            buf = io.BytesIO()
                            roc_fig.savefig(buf, format="png", dpi=120)
                            buf.seek(0)
                            st.download_button(
                                label="Descargar curva ROC",
                                data=buf,
                                file_name=f"curva_roc_{modelo['nombre']}.png",
                                mime="image/png"
                            )
                            
                        with col2:
                            # Calcular AUC para interpretaci√≥n usando funci√≥n del modelo
                            curvas_roc_data = calcular_curvas_roc_completas(y_test, y_prob, clases)
                            
                            if curvas_roc_data['es_multiclase']:
                                auc_valor = curvas_roc_data['auc_promedio']
                            else:
                                auc_valor = curvas_roc_data['auc_valor']
                            
                            interpretacion = generar_interpretacion_curva_roc(float(auc_valor))
                            st.markdown(interpretacion)
                        
                    # TAB 3: Precision-Recall
                    with tabs[2]:
                        # Determinar si es multiclase
                        es_multiclase = len(np.unique(y_test)) > 2
                        
                        # Generar y mostrar curva PR
                        pr_fig = generar_curva_precision_recall(
                            y_test, 
                            y_prob, 
                            clases=[str(c) for c in clases],  # Convertir a lista de strings
                            multi_clase=es_multiclase,
                            titulo=f"Curva Precision-Recall - {modelo['nombre']}",
                            id_sesion=session.obtener_estado("id_sesion", "sin_sesion"),
                            usuario=session.obtener_estado("usuario_id", "sistema")
                        )
                        
                        st.pyplot(pr_fig)
                        
                        # Opci√≥n para descargar
                        buf = io.BytesIO()
                        pr_fig.savefig(buf, format="png", dpi=120)
                        buf.seek(0)
                        st.download_button(
                            label="Descargar curva Precision-Recall",
                            data=buf,
                            file_name=f"curva_pr_{modelo['nombre']}.png",
                            mime="image/png"
                        )
                          # TAB 4: Comparaci√≥n de Modelos
                    with tabs[3]:
                        st.write("Seleccione modelos para comparar:")
                        
                        # Permitir selecci√≥n m√∫ltiple de modelos
                        modelos_comparar = st.multiselect(
                            "Modelos a comparar:",
                            options=nombres_modelos,
                            default=[modelo_seleccionado]
                        )
                        
                        if modelos_comparar:
                            # Preparar diccionario de modelos
                            modelos_dict = {}
                            for nombre in modelos_comparar:
                                for m in resultados_benchmarking['modelos_exitosos']:
                                    if m['nombre'] == nombre and 'modelo_objeto' in m:
                                        modelos_dict[nombre] = {"modelo": m['modelo_objeto']}
                                        break
                            
                            # Generar comparaci√≥n de curvas ROC
                            if modelos_dict:
                                comp_fig = comparar_modelos_roc(
                                    modelos_dict,
                                    X_test,
                                    y_test,
                                    titulo="Comparaci√≥n de Modelos - Curva ROC",
                                    id_sesion=session.obtener_estado("id_sesion", "sin_sesion"),
                                    usuario=session.obtener_estado("usuario_id", "sistema")
                                )
                                
                                st.pyplot(comp_fig)
                                
                                # Opci√≥n para descargar
                                buf = io.BytesIO()
                                comp_fig.savefig(buf, format="png", dpi=120)
                                buf.seek(0)
                                st.download_button(
                                    label="Descargar comparaci√≥n",
                                    data=buf,
                                    file_name="comparacion_modelos_roc.png",
                                    mime="image/png"
                                )
                            else:
                                st.warning("No se pudieron cargar los modelos seleccionados para la comparaci√≥n.")
                        else:
                            st.info("Seleccione al menos un modelo para comparar.")
                                
                else:
                    st.info("""
                    ‚ö†Ô∏è No hay datos suficientes para generar visualizaciones avanzadas. 
                    Esto puede deberse a que el modelo fue cargado desde un benchmarking anterior 
                    que no guard√≥ todos los datos necesarios.
                    
                    Intente ejecutar un nuevo benchmarking para acceder a todas las funcionalidades.
                    """)
                
            else:  # Regresi√≥n  
                col1, col2, col3, col4 = st.columns(4)
                
                # Verificar si modelo no es None y si existen las m√©tricas antes de mostrarlas
                if modelo is None:
                    st.error("No se pudo cargar el modelo seleccionado.")
                    return
                
                metricas = modelo.get('metricas', {})
                
                with col1:
                    r2_value = metricas.get('r2', 0)
                    st.metric("R¬≤", f"{r2_value:.4f}")
                
                with col2:
                    mse_value = metricas.get('mse', 0)
                    st.metric("MSE", f"{mse_value:.4f}")
                
                with col3:
                    rmse_value = metricas.get('rmse', 0)
                    st.metric("RMSE", f"{rmse_value:.4f}")
                
                with col4:
                    mae_value = metricas.get('mae', 0)
                    st.metric("MAE", f"{mae_value:.4f}")
                
                # Validaci√≥n cruzada
                st.subheader("üîÑ Validaci√≥n cruzada")
                
                # Obtener m√©tricas de validaci√≥n cruzada
                cv_score = metricas.get('cv_score_media', 0)
                cv_std = metricas.get('cv_score_std', 0)
                
                st.write(f"**Score CV (promedio):** {cv_score:.4f}")
                st.write(f"**Desviaci√≥n est√°ndar CV:** {cv_std:.4f}")
                
                # Gr√°fico de barras para m√©tricas de regresi√≥n
                fig, ax = plt.subplots(figsize=(10, 6))
                
                metricas_keys = ['r2', 'mse', 'rmse', 'mae']
                valores = [metricas.get(m, 0) for m in metricas_keys]
                
                # Gr√°fico de barras para m√©tricas de regresi√≥n
                fig, ax = plt.subplots(figsize=(10, 6))
                
                metricas = ['r2', 'mse', 'rmse', 'mae']
                valores = [modelo['metricas'][m] for m in metricas]
                
                # Para regresi√≥n, R¬≤ m√°s alto es mejor, pero MSE, RMSE, MAE m√°s bajos son mejores
                # Invertimos los valores para la visualizaci√≥n
                valores_norm = [
                    valores[0],  # R¬≤ es mejor si es m√°s alto
                    1 / (valores[1] + 1e-10),  # MSE es mejor si es m√°s bajo
                    1 / (valores[2] + 1e-10),  # RMSE es mejor si es m√°s bajo                    
                    1 / (valores[3] + 1e-10)   # MAE es mejor si es m√°s bajo
                ]
                
                # Normalizar para visualizaci√≥n
                max_val = max(valores_norm)
                valores_norm = [v/max_val for v in valores_norm]
                
                # Asignar colores simples
                colores = ['green', 'red', 'orange', 'blue']
                
                barras = ax.bar(metricas, valores, color=colores)
                
                # A√±adir etiquetas con valores originales
                for i, barra in enumerate(barras):
                    ax.text(
                        barra.get_x() + barra.get_width()/2,
                        barra.get_height() + 0.01,
                        f'{valores[i]:.4f}',
                        ha='center'
                    )
                
                # Configurar gr√°fico
                plt.title('M√©tricas de regresi√≥n')
                plt.ylabel('Valor')
                plt.grid(axis='y', linestyle='--', alpha=0.7)
                
                st.pyplot(fig)            
                
                # VISUALIZACIONES AVANZADAS PARA REGRESI√ìN
                st.subheader("üìà Visualizaciones avanzadas")
                
                # Realizar diagn√≥stico de requisitos para visualizaciones
                diagnostico = diagnosticar_visualizaciones(resultados_benchmarking, modelo)
                
                # Mostrar estado de diagn√≥stico
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    # Mostrar estado general
                    if diagnostico["puede_visualizar"]:
                        st.success("‚úÖ Todos los requisitos para visualizaciones avanzadas est√°n disponibles")
                    else:
                        st.warning("‚ö†Ô∏è Hay limitaciones para las visualizaciones avanzadas")
                
                with col2:
                    # Mostrar detalles de los requisitos
                    requisitos = diagnostico["requisitos"]
                    
                    # Datos de prueba
                    if requisitos.get("datos_prueba", False):
                        st.write("‚úÖ Datos de prueba disponibles")
                    else:
                        st.write("‚ùå Datos de prueba no disponibles")
                    
                    # Objeto del modelo
                    if requisitos.get("modelo_objeto", False):
                        st.write("‚úÖ Objeto del modelo disponible")
                    else:
                        st.write("‚ùå Objeto del modelo no disponible")
                    
                    # M√©tricas
                    if requisitos.get("metricas", False):
                        st.write("‚úÖ M√©tricas completas disponibles")
                    else:
                        st.write("‚ùå M√©tricas incompletas")
                
                # Mostrar recomendaciones si hay problemas
                if diagnostico["recomendaciones"]:
                    with st.expander("üìã Recomendaciones para mejorar visualizaciones", expanded=True):
                        for recomendacion in diagnostico["recomendaciones"]:
                            st.info(recomendacion)
                
                # Comprobar si tenemos los datos para las visualizaciones
                if 'X_test' in resultados_benchmarking and 'y_test' in resultados_benchmarking:
                    try:
                        # Convertir datos serializados de vuelta a numpy arrays si es necesario
                        X_test = np.array(resultados_benchmarking['X_test']) if isinstance(resultados_benchmarking['X_test'], list) else resultados_benchmarking['X_test']
                        y_test = np.array(resultados_benchmarking['y_test']) if isinstance(resultados_benchmarking['y_test'], list) else resultados_benchmarking['y_test']
                        
                        # Obtener predicciones si est√° disponible el modelo
                        if 'modelo_objeto' in modelo:
                            # Obtener predicciones
                            y_pred = modelo['modelo_objeto'].predict(X_test)
                        else:
                            # Si no hay modelo, usar resultados pre-calculados o dummy data
                            st.warning("El modelo no est√° disponible para generar predicciones en tiempo real.")
                            y_pred = y_test * 0.9 + np.random.normal(0, 0.1, len(y_test))  # Fallback con algo de ruido
                    
                        # Mostrar visualizaciones simplificadas basadas en m√©tricas disponibles
                        st.info("Mostrando visualizaciones basadas en las m√©tricas calculadas durante el entrenamiento.")
                        
                        # Gr√°fico comparativo de m√©tricas de regresi√≥n
                        fig, ax = plt.subplots(figsize=(10, 6))
                        
                        metricas = ['r2', 'mse', 'rmse', 'mae']
                        valores = [modelo['metricas'][m] for m in metricas]
                        
                        # Visualizar valores directamente (sin normalizaci√≥n)
                        ax.bar(metricas, valores, color=['green', 'red', 'orange', 'blue'])
                        for i, v in enumerate(valores):
                            ax.text(i, v, f'{v:.3f}', ha='center', va='bottom')
                        
                        ax.set_title(f'M√©tricas de regresi√≥n para {modelo["nombre"]}')
                        ax.set_ylabel('Valor')
                        ax.grid(axis='y', linestyle='--', alpha=0.7)
                        
                        st.pyplot(fig)
                        
                        # Pesta√±as para diferentes visualizaciones
                        tabs = st.tabs([
                            "Gr√°fico de Residuos",
                            "Comparaci√≥n de Distribuciones", 
                            "Comparar Modelos"
                        ])
                        
                        # TAB 1: Gr√°fico de Residuos
                        with tabs[0]:
                            col1, col2 = st.columns([2, 1])
                            
                            with col1:
                                if 'modelo_objeto' in modelo:
                                    # Generar gr√°fico de residuos
                                    residuos_fig = generar_grafico_residuos(
                                        y_test, 
                                        y_pred, 
                                        titulo=f"An√°lisis de Residuos - {modelo['nombre']}",
                                        id_sesion=session.obtener_estado("id_sesion", "sin_sesion"),
                                        usuario=session.obtener_estado("usuario_id", "sistema")
                                    )
                                    
                                    st.pyplot(residuos_fig)
                                    
                                    # Opci√≥n para descargar
                                    buf = io.BytesIO()
                                    residuos_fig.savefig(buf, format="png", dpi=120)
                                    buf.seek(0)
                                    st.download_button(
                                        label="Descargar gr√°fico de residuos",
                                        data=buf,
                                        file_name=f"residuos_{modelo['nombre']}.png",
                                        mime="image/png"
                                    )
                                else:
                                    st.warning("No hay datos suficientes para generar el gr√°fico de residuos.")
                            
                            with col2:
                                # Generar interpretaci√≥n de residuos
                                if 'modelo_objeto' in modelo:
                                    try:
                                        # Calcular residuos
                                        residuos = y_test - y_pred
                                        interpretacion = generar_interpretacion_residuos(residuos)
                                        st.markdown(interpretacion)
                                    except Exception as e:
                                        st.error(f"Error al calcular residuos: {str(e)}")
                                        log_audit(
                                            session.obtener_estado("usuario_id", "sistema"),
                                            "ERROR_CALCULO_RESIDUOS",
                                            "Evaluar_Modelos",
                                            modelo['nombre'] if modelo else "N/A",
                                            f"Error al calcular residuos: {str(e)}",
                                            id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                                        )
                        
                        # TAB 2: Comparaci√≥n de Distribuciones
                        with tabs[1]:
                            if 'modelo_objeto' in modelo:
                                try:
                                    # Generar y mostrar comparaci√≥n de distribuciones
                                    dist_fig = comparar_distribuciones(
                                        y_test, 
                                        y_pred, 
                                        titulo=f"Comparaci√≥n de Distribuciones - {modelo['nombre']}",
                                        id_sesion=session.obtener_estado("id_sesion", "sin_sesion"),
                                        usuario=session.obtener_estado("usuario_id", "sistema")
                                    )
                                    
                                    st.pyplot(dist_fig)
                                    
                                    # Opci√≥n para descargar
                                    buf = io.BytesIO()
                                    dist_fig.savefig(buf, format="png", dpi=120)
                                    buf.seek(0)
                                    st.download_button(
                                        label="Descargar comparaci√≥n de distribuciones",
                                        data=buf,
                                        file_name=f"distribuciones_{modelo['nombre']}.png",
                                        mime="image/png"
                                    )
                                except Exception as e:
                                    st.error(f"Error al generar visualizaci√≥n de distribuciones: {str(e)}")
                                    log_audit(
                                        session.obtener_estado("usuario_id", "sistema"),
                                        "ERROR_VISUALIZACIONES",
                                        "Evaluar_Modelos",
                                        modelo['nombre'] if modelo else "N/A",
                                        f"Error al generar visualizaci√≥n de distribuciones: {str(e)}",
                                        id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                                    )
                            else:
                                st.warning("No hay datos suficientes para generar la comparaci√≥n de distribuciones.")
                                    
                        # TAB 3: Comparar Modelos
                        with tabs[2]:
                            st.write("Seleccione modelos para comparar:")
                            
                            # Permitir selecci√≥n m√∫ltiple de modelos
                            modelos_comparar = st.multiselect(
                                "Modelos a comparar:",
                                options=nombres_modelos,
                                default=[modelo_seleccionado]
                            )
                            
                            if modelos_comparar:
                                # Preparar diccionario de modelos
                                modelos_dict = {}
                                
                                for nombre in modelos_comparar:
                                    for m in resultados_benchmarking['modelos_exitosos']:
                                        if m['nombre'] == nombre and 'modelo_objeto' in m:
                                            modelos_dict[nombre] = {"modelo": m['modelo_objeto']}
                                            break
                                
                                if modelos_dict:
                                    try:
                                        # Usar funci√≥n del modelo para crear datos de comparaci√≥n
                                        datos_comparacion = generar_datos_grafico_comparacion_regresion(
                                            modelos_dict, X_test, y_test
                                        )
                                        
                                        # Mostrar gr√°fico de dispersi√≥n
                                        fig, ax = plt.subplots(figsize=(10, 6))
                                        
                                        # Graficar l√≠nea de referencia (predicci√≥n perfecta)
                                        rango = datos_comparacion['rango_referencia']
                                        ax.plot([rango['min'], rango['max']], [rango['min'], rango['max']], 
                                                'k--', label='Predicci√≥n perfecta')
                                        
                                        # Graficar predicciones de cada modelo usando datos del modelo
                                        for nombre, datos in datos_comparacion['datos_modelos'].items():
                                            ax.scatter(datos['x'], datos['y'], label=nombre)
                                        
                                        # A√±adir etiquetas y leyenda
                                        ax.set_xlabel('Valores reales')
                                        ax.set_ylabel('Valores predichos')
                                        ax.set_title('Comparaci√≥n de predicciones')
                                        ax.legend()
                                        
                                        # Mostrar el gr√°fico
                                        st.pyplot(fig)
                                        
                                        # Mostrar tabla de m√©tricas comparativas
                                        st.subheader("M√©tricas comparativas")
                                        
                                        buf = io.BytesIO()
                                        fig.savefig(buf, format="png", dpi=120)
                                        buf.seek(0)
                                        st.download_button(
                                            label="Descargar comparaci√≥n",
                                            data=buf,
                                            file_name="comparacion_modelos_regresion.png",
                                            mime="image/png"
                                        )
                                        
                                        # Usar funci√≥n del modelo para comparar m√©tricas
                                        metricas_comp = comparar_metricas_regresion(modelos_dict, X_test, y_test)
                                        
                                        # Mostrar la tabla de m√©tricas comparativas
                                        st.dataframe(metricas_comp)
                                        
                                    except Exception as e:
                                        st.error(f"Error al comparar modelos: {str(e)}")
                                        log_audit(
                                            session.obtener_estado("usuario_id", "sistema"),
                                            "ERROR_COMPARACION_MODELOS",
                                            "Evaluar_Modelos",
                                            modelo['nombre'] if modelo else "N/A",
                                            f"Error al comparar modelos: {str(e)}",
                                            id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                                        )
                                else:
                                    st.warning("No se pudieron cargar los modelos seleccionados para la comparaci√≥n.")
                            else:
                                st.info("Seleccione al menos un modelo para comparar.")
                    
                    except Exception as e:
                        st.error(f"Error al generar visualizaciones: {str(e)}")
                        log_audit(
                            session.obtener_estado("usuario_id", "sistema"),
                            "ERROR_VISUALIZACION",
                            "Evaluar_Modelos",
                            modelo['nombre'] if modelo else "N/A",
                            f"Error al generar visualizaciones para {modelo['nombre'] if modelo else 'N/A'}: {str(e)}",
                            id_sesion=session.obtener_estado("id_sesion", "sin_sesion")
                        )
                else:
                    st.info("""
                        ‚ö†Ô∏è No hay datos suficientes para generar visualizaciones avanzadas. 
                        Esto puede deberse a que el modelo fue cargado desde un benchmarking anterior 
                        que no guard√≥ todos los datos necesarios.
                        
                        Intente ejecutar un nuevo benchmarking para acceder a todas las funcionalidades.
                        """)
                               
            # Opciones adicionales
            st.subheader("‚è© Pr√≥ximos pasos")
            
            # Botones para navegaci√≥n
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("üîô Volver a Entrenar Modelos", use_container_width=True):
                    # Guardar instrucci√≥n de navegaci√≥n en la sesi√≥n
                    session.guardar_estado("navegacion", "Entrenar_Modelos")
                    # Redirigir a la p√°gina de entrenamiento
                    st.switch_page("pages/Machine Learning/05_Entrenar_Modelos.py")
            
            with col2:
                if st.button("üß† Validaci√≥n Cruzada", use_container_width=True):
                    # Guardar instrucci√≥n de navegaci√≥n en la sesi√≥n
                    session.guardar_estado("navegacion", "Validacion_Cruzada")
                    # Redirigir a la p√°gina de validaci√≥n cruzada
                    st.switch_page("pages/Machine Learning/07_Validacion_Cruzada.py")
            
            with col3:
                if st.button("üëë Recomendar Modelo", use_container_width=True):
                    # Guardar instrucci√≥n de navegaci√≥n en la sesi√≥n
                    session.guardar_estado("navegacion", "Recomendar_Modelo")
                    # Redirigir a la p√°gina de recomendaci√≥n
                    st.switch_page("pages/Machine Learning/08_Recomendar_Modelo.py")
    else:
        st.warning("No hay modelos exitosos para evaluar.")

if __name__ == "__main__":
    main()